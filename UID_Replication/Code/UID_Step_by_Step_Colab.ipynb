{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZ8j0jzdEYtd"
      },
      "source": [
        "# UID Paper Replication - Step by Step\n",
        "\n",
        "Name: Ashish Pisey\n",
        "Roll no.: M25Ai2117"
      ],
      "id": "zZ8j0jzdEYtd"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OJmc-YdlEYte",
        "outputId": "27830874-69ec-4ca6-b803-ec50b5c3ab28",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# STEP 0: Mount Google Drive and set paths\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "TRAIN_PATH = '/content/drive/MyDrive/MTP/Projects/UID_replication/Datasets/hi_hdtb-ud-train.conllu'\n",
        "DEV_PATH = '/content/drive/MyDrive/MTP/Projects/UID_replication/Datasets/hi_hdtb-ud-dev.conllu'\n",
        "TEST_PATH = '/content/drive/MyDrive/MTP/Projects/UID_replication/Datasets/hi_hdtb-ud-test.conllu'\n",
        "\n",
        "# For testing with smaller data, limit sentences\n",
        "MAX_TRAIN = 5000  # Set to None for all\n",
        "MAX_TEST = 500    # Set to None for all\n",
        "MAX_VARIANTS = 20  # Variants per sentence"
      ],
      "id": "OJmc-YdlEYte"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "i6V1jBEoEYtg"
      },
      "outputs": [],
      "source": [
        "# STEP 1: Import required libraries\n",
        "\n",
        "import os\n",
        "import math\n",
        "import random\n",
        "from collections import defaultdict, Counter\n",
        "from itertools import permutations\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "random.seed(42)\n",
        "np.random.seed(42)"
      ],
      "id": "i6V1jBEoEYtg"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "E3s17L0NEYth"
      },
      "outputs": [],
      "source": [
        "# STEP 2: Define data structures for tokens and sentences\n",
        "\n",
        "class Token:\n",
        "    def __init__(self, id, form, lemma, upos, xpos, feats, head, deprel, deps, misc):\n",
        "        self.id = id\n",
        "        self.form = form\n",
        "        self.lemma = lemma\n",
        "        self.upos = upos\n",
        "        self.xpos = xpos\n",
        "        self.feats = feats\n",
        "        self.head = head\n",
        "        self.deprel = deprel\n",
        "        self.deps = deps\n",
        "        self.misc = misc\n",
        "\n",
        "class Sentence:\n",
        "    def __init__(self, sent_id, text, tokens):\n",
        "        self.sent_id = sent_id\n",
        "        self.text = text\n",
        "        self.tokens = tokens\n",
        "\n",
        "    def get_root(self):\n",
        "        for t in self.tokens:\n",
        "            if t.head == 0:\n",
        "                return t\n",
        "        return None\n",
        "\n",
        "    def get_word_sequence(self):\n",
        "        sorted_tokens = sorted(self.tokens, key=lambda x: x.id)\n",
        "        return [t.form for t in sorted_tokens]\n",
        "\n",
        "    def get_preverbal_constituents(self):\n",
        "        root = self.get_root()\n",
        "        if not root:\n",
        "            return []\n",
        "        preverbal = [t for t in self.tokens if t.id < root.id]\n",
        "        constituents = defaultdict(list)\n",
        "        for t in preverbal:\n",
        "            constituents[t.head].append(t)\n",
        "        result = []\n",
        "        for head_id in sorted(constituents.keys()):\n",
        "            result.append(sorted(constituents[head_id], key=lambda x: x.id))\n",
        "        return result"
      ],
      "id": "E3s17L0NEYth"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "NOj-T-qnEYth"
      },
      "outputs": [],
      "source": [
        "# STEP 3: Parse CoNLL-U files\n",
        "\n",
        "def parse_feats(feats_str):\n",
        "    feats = {}\n",
        "    if feats_str and feats_str != '_':\n",
        "        for feat in feats_str.split('|'):\n",
        "            if '=' in feat:\n",
        "                key, value = feat.split('=', 1)\n",
        "                feats[key] = value\n",
        "    return feats\n",
        "\n",
        "def parse_conllu(filepath, max_sent=None):\n",
        "    sentences = []\n",
        "    current_tokens = []\n",
        "    sent_id = ''\n",
        "    text = ''\n",
        "\n",
        "    with open(filepath, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "\n",
        "            if line.startswith('# sent_id'):\n",
        "                sent_id = line.split('=', 1)[1].strip() if '=' in line else ''\n",
        "            elif line.startswith('# text'):\n",
        "                text = line.split('=', 1)[1].strip() if '=' in line else ''\n",
        "            elif line == '':\n",
        "                if current_tokens:\n",
        "                    sentences.append(Sentence(sent_id, text, current_tokens))\n",
        "                    current_tokens = []\n",
        "                    sent_id = ''\n",
        "                    text = ''\n",
        "                    if max_sent and len(sentences) >= max_sent:\n",
        "                        break\n",
        "            else:\n",
        "                fields = line.split('\\t')\n",
        "                if len(fields) == 10 and '-' not in fields[0] and '.' not in fields[0]:\n",
        "                    token = Token(\n",
        "                        id=int(fields[0]),\n",
        "                        form=fields[1],\n",
        "                        lemma=fields[2],\n",
        "                        upos=fields[3],\n",
        "                        xpos=fields[4],\n",
        "                        feats=parse_feats(fields[5]),\n",
        "                        head=int(fields[6]) if fields[6] != '_' else 0,\n",
        "                        deprel=fields[7],\n",
        "                        deps=fields[8],\n",
        "                        misc=fields[9]\n",
        "                    )\n",
        "                    current_tokens.append(token)\n",
        "\n",
        "    if current_tokens:\n",
        "        sentences.append(Sentence(sent_id, text, current_tokens))\n",
        "\n",
        "    return sentences"
      ],
      "id": "NOj-T-qnEYth"
    },
    {
      "cell_type": "code",
      "source": [
        "# CoNLL-U parsing with `conllu` library (reference implementation)\n",
        "\n",
        "import sys\n",
        "import subprocess\n",
        "\n",
        "try:\n",
        "    from conllu import parse_incr\n",
        "except ImportError:\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'conllu'])\n",
        "    from conllu import parse_incr\n",
        "\n",
        "\n",
        "def parse_conllu_with_conllu(filepath, max_sent=None):\n",
        "    sentences = []\n",
        "\n",
        "    with open(filepath, 'r', encoding='utf-8') as f:\n",
        "        for i, tokenlist in enumerate(parse_incr(f)):\n",
        "            sent_id = tokenlist.metadata.get('sent_id', '')\n",
        "            text = tokenlist.metadata.get('text', '')\n",
        "            tokens = []\n",
        "\n",
        "            for tok in tokenlist:\n",
        "                tok_id = tok.get('id')\n",
        "                # Skip multiword tokens and empty nodes to match our parser behavior.\n",
        "                if not isinstance(tok_id, int):\n",
        "                    continue\n",
        "\n",
        "                head = tok.get('head')\n",
        "                tokens.append(Token(\n",
        "                    id=tok_id,\n",
        "                    form=tok.get('form', ''),\n",
        "                    lemma=tok.get('lemma', '_'),\n",
        "                    upos=tok.get('upos', '_'),\n",
        "                    xpos=tok.get('xpos', '_'),\n",
        "                    feats=dict(tok.get('feats') or {}),\n",
        "                    head=head if isinstance(head, int) else 0,\n",
        "                    deprel=tok.get('deprel', '_'),\n",
        "                    deps=tok.get('deps', '_'),\n",
        "                    misc=tok.get('misc', '_'),\n",
        "                ))\n",
        "\n",
        "            if tokens:\n",
        "                sentences.append(Sentence(sent_id, text, tokens))\n",
        "\n",
        "            if max_sent and len(sentences) >= max_sent:\n",
        "                break\n",
        "\n",
        "    return sentences\n",
        "\n",
        "# Quick demo (parses a few sentences if one of the configured paths exists)\n",
        "_demo_path = next((p for p in [globals().get('TEST_PATH'), globals().get('DEV_PATH'), globals().get('TRAIN_PATH')] if isinstance(p, str) and os.path.exists(p)), None)\n",
        "if _demo_path:\n",
        "    _demo_sents = parse_conllu_with_conllu(_demo_path, max_sent=2)\n",
        "    print(f'conllu parser demo: parsed {len(_demo_sents)} sentence(s) from {_demo_path}')\n",
        "    if _demo_sents:\n",
        "        print(f'  First sent_id: {_demo_sents[0].sent_id}')\n",
        "        print(f'  First text: {_demo_sents[0].text}')\n",
        "        print(f'  First words: {_demo_sents[0].get_word_sequence()[:15]}')\n",
        "else:\n",
        "    print('conllu parser cell ready. Set TRAIN_PATH/DEV_PATH/TEST_PATH to run the demo.')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0MQdOzaoIH43",
        "outputId": "cc0150ef-384d-457f-f7c2-35fd49018f1a"
      },
      "id": "0MQdOzaoIH43",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "conllu parser demo: parsed 2 sentence(s) from /content/drive/MyDrive/MTP/Projects/UID_replication/Datasets/hi_hdtb-ud-test.conllu\n",
            "  First sent_id: test-s1\n",
            "  First text: इसके अतिरिक्त गुग्गुल कुंड, भीम गुफा तथा भीमशिला भी दर्शनीय स्थल हैं ।\n",
            "  First words: ['इसके', 'अतिरिक्त', 'गुग्गुल', 'कुंड', ',', 'भीम', 'गुफा', 'तथा', 'भीमशिला', 'भी', 'दर्शनीय', 'स्थल', 'हैं', '।']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "gS_r_7vXEYth",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40843105-721e-4d29-a4ed-8734291e0c45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading training data...\n",
            "Loaded 5000 training sentences\n",
            "Loading dev data...\n",
            "Loaded 1659 dev sentences\n",
            "Loading test data...\n",
            "Loaded 500 test sentences\n",
            "Total: 7159 sentences\n"
          ]
        }
      ],
      "source": [
        "# STEP 4: Loading dataset\n",
        "\n",
        "print('Loading training data...')\n",
        "train_sentences = parse_conllu(TRAIN_PATH, MAX_TRAIN)\n",
        "print(f'Loaded {len(train_sentences)} training sentences')\n",
        "\n",
        "print('Loading dev data...')\n",
        "dev_sentences = parse_conllu(DEV_PATH)\n",
        "print(f'Loaded {len(dev_sentences)} dev sentences')\n",
        "\n",
        "print('Loading test data...')\n",
        "test_sentences = parse_conllu(TEST_PATH, MAX_TEST)\n",
        "print(f'Loaded {len(test_sentences)} test sentences')\n",
        "\n",
        "all_sentences = train_sentences + dev_sentences + test_sentences\n",
        "print(f'Total: {len(all_sentences)} sentences')"
      ],
      "id": "gS_r_7vXEYth"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "IHAT6mwVEYti",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31f0bfbd-9b3d-4db4-ae1f-fc0a57ddad28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training trigram model...\n",
            "Vocabulary size: 10294\n",
            "Total tokens: 101712\n",
            "Unigrams: 10294\n",
            "Bigrams: 52111\n",
            "Trigrams: 81145\n"
          ]
        }
      ],
      "source": [
        "# STEP 5: Build trigram model for lexical surprisal\n",
        "\n",
        "unigram_counts = Counter()\n",
        "bigram_counts = Counter()\n",
        "trigram_counts = Counter()\n",
        "vocab = set()\n",
        "total_tokens = 0\n",
        "\n",
        "print('Training trigram model...')\n",
        "\n",
        "for sentence in train_sentences:\n",
        "    words = sentence.get_word_sequence()\n",
        "    tokens = ['<s>', '<s>'] + words + ['</s>']\n",
        "\n",
        "    for i in range(len(tokens)):\n",
        "        unigram_counts[tokens[i]] += 1\n",
        "        vocab.add(tokens[i])\n",
        "\n",
        "        if i > 0:\n",
        "            bigram_counts[(tokens[i-1], tokens[i])] += 1\n",
        "\n",
        "        if i > 1:\n",
        "            trigram_counts[(tokens[i-2], tokens[i-1], tokens[i])] += 1\n",
        "\n",
        "    total_tokens += len(words)\n",
        "\n",
        "vocab_size = len(vocab)\n",
        "print(f'Vocabulary size: {vocab_size}')\n",
        "print(f'Total tokens: {total_tokens}')\n",
        "print(f'Unigrams: {len(unigram_counts)}')\n",
        "print(f'Bigrams: {len(bigram_counts)}')\n",
        "print(f'Trigrams: {len(trigram_counts)}')"
      ],
      "id": "IHAT6mwVEYti"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "CixBaKTEEYti"
      },
      "outputs": [],
      "source": [
        "# STEP 6: Function to compute lexical surprisal\n",
        "\n",
        "def get_trigram_prob(word, prev1, prev2):\n",
        "    lambda3 = 0.6\n",
        "    lambda2 = 0.3\n",
        "    lambda1 = 0.1\n",
        "\n",
        "    trigram_count = trigram_counts.get((prev2, prev1, word), 0)\n",
        "    bigram_context = bigram_counts.get((prev2, prev1), 0)\n",
        "    p_trigram = trigram_count / bigram_context if bigram_context > 0 else 0\n",
        "\n",
        "    bigram_count = bigram_counts.get((prev1, word), 0)\n",
        "    unigram_context = unigram_counts.get(prev1, 0)\n",
        "    p_bigram = bigram_count / unigram_context if unigram_context > 0 else 0\n",
        "\n",
        "    p_unigram = (unigram_counts.get(word, 0) + 1) / (total_tokens + vocab_size)\n",
        "\n",
        "    prob = lambda3 * p_trigram + lambda2 * p_bigram + lambda1 * p_unigram\n",
        "    return max(prob, 1e-10)\n",
        "\n",
        "def compute_lexical_surprisal(word_sequence):\n",
        "    tokens = ['<s>', '<s>'] + word_sequence + ['</s>']\n",
        "    surprisals = []\n",
        "\n",
        "    for i in range(2, len(tokens)):\n",
        "        prob = get_trigram_prob(tokens[i], tokens[i-1], tokens[i-2])\n",
        "        surprisal = -math.log2(prob)\n",
        "        surprisals.append(surprisal)\n",
        "\n",
        "    return surprisals"
      ],
      "id": "CixBaKTEEYti"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "EA0EsFCbEYtj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1100624e-54d4-496d-d0c4-dcf082f5d920"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training syntactic model...\n",
            "Unique states: 2347\n",
            "Unique transitions: 12203\n"
          ]
        }
      ],
      "source": [
        "# STEP 7: Build syntactic surprisal model\n",
        "\n",
        "state_counts = Counter()\n",
        "transition_counts = Counter()\n",
        "total_transitions = 0\n",
        "\n",
        "print('Training syntactic model...')\n",
        "\n",
        "def get_state(tokens, idx):\n",
        "    if idx < 0:\n",
        "        return 'START'\n",
        "    parts = []\n",
        "    for i in range(max(0, idx-1), idx+1):\n",
        "        parts.append(f\"{tokens[i].upos}:{tokens[i].deprel}\")\n",
        "    return '|'.join(parts)\n",
        "\n",
        "for sentence in train_sentences:\n",
        "    tokens = sorted(sentence.tokens, key=lambda x: x.id)\n",
        "\n",
        "    for i, token in enumerate(tokens):\n",
        "        state = get_state(tokens, i)\n",
        "        state_counts[state] += 1\n",
        "\n",
        "        if i > 0:\n",
        "            prev_state = get_state(tokens, i-1)\n",
        "            transition_counts[(prev_state, state)] += 1\n",
        "            total_transitions += 1\n",
        "\n",
        "print(f'Unique states: {len(state_counts)}')\n",
        "print(f'Unique transitions: {len(transition_counts)}')"
      ],
      "id": "EA0EsFCbEYtj"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "admnZUaREYtj"
      },
      "outputs": [],
      "source": [
        "# STEP 8: Function to compute syntactic surprisal\n",
        "\n",
        "def get_state_prob(tokens, idx):\n",
        "    state = get_state(tokens, idx)\n",
        "    count = state_counts.get(state, 0)\n",
        "    return (count + 1) / (total_transitions + len(state_counts) + 1)\n",
        "\n",
        "def compute_syntactic_surprisal(sentence):\n",
        "    tokens = sorted(sentence.tokens, key=lambda x: x.id)\n",
        "    surprisals = []\n",
        "\n",
        "    for i in range(len(tokens)):\n",
        "        if i == 0:\n",
        "            surprisals.append(0.0)\n",
        "        else:\n",
        "            prev_prob = get_state_prob(tokens, i-1)\n",
        "            curr_prob = get_state_prob(tokens, i)\n",
        "            ratio = curr_prob / max(prev_prob, 1e-10)\n",
        "            surprisal = -math.log2(ratio)\n",
        "            surprisals.append(max(0, surprisal))\n",
        "\n",
        "    return surprisals"
      ],
      "id": "admnZUaREYtj"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "IDfSfwwzEYtj"
      },
      "outputs": [],
      "source": [
        "# STEP 9: Define UID measures\n",
        "\n",
        "def mean_info(info_list):\n",
        "    return sum(info_list) / len(info_list) if info_list else 0\n",
        "\n",
        "def uid_global(info_list):\n",
        "    if len(info_list) <= 1:\n",
        "        return 0\n",
        "    m = mean_info(info_list)\n",
        "    variance = sum((x - m) ** 2 for x in info_list) / len(info_list)\n",
        "    return -variance\n",
        "\n",
        "def uid_local(info_list):\n",
        "    if len(info_list) <= 1:\n",
        "        return 0\n",
        "    diffs = [(info_list[i] - info_list[i-1]) ** 2 for i in range(1, len(info_list))]\n",
        "    return -sum(diffs) / len(info_list)\n",
        "\n",
        "def uid_global_norm(info_list):\n",
        "    if len(info_list) <= 1:\n",
        "        return 0\n",
        "    m = mean_info(info_list)\n",
        "    if m == 0:\n",
        "        return 0\n",
        "    return -sum(((x / m) - 1) ** 2 for x in info_list) / len(info_list)\n",
        "\n",
        "def uid_local_norm(info_list):\n",
        "    if len(info_list) <= 1:\n",
        "        return 0\n",
        "    m = mean_info(info_list)\n",
        "    if m == 0:\n",
        "        return 0\n",
        "    diffs = [((info_list[i] - info_list[i-1]) ** 2) / (m ** 2) for i in range(1, len(info_list))]\n",
        "    return -sum(diffs) / len(info_list)\n",
        "\n",
        "def uid_local_prev_norm(info_list):\n",
        "    if len(info_list) <= 1:\n",
        "        return 0\n",
        "    diffs = []\n",
        "    for i in range(1, len(info_list)):\n",
        "        if info_list[i-1] != 0:\n",
        "            diffs.append(((info_list[i] / info_list[i-1]) - 1) ** 2)\n",
        "    return -sum(diffs) / len(info_list) if diffs else 0\n",
        "\n",
        "def compute_all_uid(info_list):\n",
        "    return {\n",
        "        'UIDglob': uid_global(info_list),\n",
        "        'UIDloc': uid_local(info_list),\n",
        "        'UIDglobNorm': uid_global_norm(info_list),\n",
        "        'UIDlocNorm': uid_local_norm(info_list),\n",
        "        'UIDlocPrevNorm': uid_local_prev_norm(info_list)\n",
        "    }"
      ],
      "id": "IDfSfwwzEYtj"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "yVT0Mu1xEYtk"
      },
      "outputs": [],
      "source": [
        "# STEP 10: Generate variants by permuting preverbal constituents\n",
        "\n",
        "def generate_variant(sentence, permutation):\n",
        "    constituents = sentence.get_preverbal_constituents()\n",
        "    root = sentence.get_root()\n",
        "    postverbal = [t for t in sentence.tokens if root and t.id > root.id]\n",
        "\n",
        "    new_tokens = []\n",
        "    new_id = 1\n",
        "\n",
        "    for const_idx in permutation:\n",
        "        if const_idx < len(constituents):\n",
        "            for t in constituents[const_idx]:\n",
        "                new_tokens.append(Token(new_id, t.form, t.lemma, t.upos, t.xpos, t.feats, 0, t.deprel, t.deps, t.misc))\n",
        "                new_id += 1\n",
        "\n",
        "    if root:\n",
        "        new_tokens.append(Token(new_id, root.form, root.lemma, root.upos, root.xpos, root.feats, 0, root.deprel, root.deps, root.misc))\n",
        "        new_id += 1\n",
        "\n",
        "    for t in postverbal:\n",
        "        new_tokens.append(Token(new_id, t.form, t.lemma, t.upos, t.xpos, t.feats, t.head, t.deprel, t.deps, t.misc))\n",
        "        new_id += 1\n",
        "\n",
        "    text = ' '.join([t.form for t in new_tokens])\n",
        "    return Sentence(f\"{sentence.sent_id}_var\", text, new_tokens)\n",
        "\n",
        "def generate_variants(sentence, max_var=99):\n",
        "    constituents = sentence.get_preverbal_constituents()\n",
        "    n_const = len(constituents)\n",
        "    if n_const <= 1:\n",
        "        return []\n",
        "\n",
        "    original = tuple(range(n_const))\n",
        "    total_alternatives = math.factorial(n_const) - 1\n",
        "    selected_perms = []\n",
        "\n",
        "    # Avoid materializing all permutations; this can explode RAM for long sentences.\n",
        "    if total_alternatives <= max_var:\n",
        "        for p in permutations(range(n_const)):\n",
        "            if p != original:\n",
        "                selected_perms.append(p)\n",
        "    else:\n",
        "        seen = {original}\n",
        "        while len(selected_perms) < max_var:\n",
        "            p = list(range(n_const))\n",
        "            random.shuffle(p)\n",
        "            p = tuple(p)\n",
        "            if p in seen:\n",
        "                continue\n",
        "            seen.add(p)\n",
        "            selected_perms.append(p)\n",
        "\n",
        "    return [generate_variant(sentence, p) for p in selected_perms]\n"
      ],
      "id": "yVT0Mu1xEYtk"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "1qYq_BCuEYtk"
      },
      "outputs": [],
      "source": [
        "# STEP 11: Extract features for a sentence\n",
        "\n",
        "def extract_features(sentence):\n",
        "    features = {}\n",
        "\n",
        "    words = sentence.get_word_sequence()\n",
        "    lex_surp = compute_lexical_surprisal(words)\n",
        "    syn_surp = compute_syntactic_surprisal(sentence)\n",
        "\n",
        "    features['lex_sum'] = sum(lex_surp)\n",
        "    features['syn_sum'] = sum(syn_surp)\n",
        "\n",
        "    lex_uid = compute_all_uid(lex_surp)\n",
        "    syn_uid = compute_all_uid(syn_surp)\n",
        "\n",
        "    for k, v in lex_uid.items():\n",
        "        features[f'lex_{k}'] = v\n",
        "    for k, v in syn_uid.items():\n",
        "        features[f'syn_{k}'] = v\n",
        "\n",
        "    return features"
      ],
      "id": "1qYq_BCuEYtk"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "E5HBv_I3EYtk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6fd2e7a-3ee2-42e9-ff81-56e6f37c1c26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating variants and extracting features...\n",
            "Processed 20/500 sentences, 198 pairs\n",
            "Processed 30/500 sentences, 353 pairs\n",
            "Processed 40/500 sentences, 503 pairs\n",
            "Processed 50/500 sentences, 635 pairs\n",
            "Processed 60/500 sentences, 781 pairs\n",
            "Processed 70/500 sentences, 947 pairs\n",
            "Processed 80/500 sentences, 1075 pairs\n",
            "Processed 90/500 sentences, 1260 pairs\n",
            "Processed 100/500 sentences, 1391 pairs\n",
            "Processed 120/500 sentences, 1712 pairs\n",
            "Processed 130/500 sentences, 1805 pairs\n",
            "Processed 140/500 sentences, 1913 pairs\n",
            "Processed 150/500 sentences, 2063 pairs\n",
            "Processed 160/500 sentences, 2208 pairs\n",
            "Processed 170/500 sentences, 2331 pairs\n",
            "Processed 180/500 sentences, 2516 pairs\n",
            "Processed 190/500 sentences, 2613 pairs\n",
            "Processed 200/500 sentences, 2774 pairs\n",
            "Processed 210/500 sentences, 2936 pairs\n",
            "Processed 220/500 sentences, 3136 pairs\n",
            "Processed 240/500 sentences, 3363 pairs\n",
            "Processed 250/500 sentences, 3505 pairs\n",
            "Processed 260/500 sentences, 3667 pairs\n",
            "Processed 270/500 sentences, 3837 pairs\n",
            "Processed 280/500 sentences, 3944 pairs\n",
            "Processed 300/500 sentences, 4153 pairs\n",
            "Processed 310/500 sentences, 4228 pairs\n",
            "Processed 320/500 sentences, 4413 pairs\n",
            "Processed 330/500 sentences, 4544 pairs\n",
            "Processed 340/500 sentences, 4654 pairs\n",
            "Processed 350/500 sentences, 4804 pairs\n",
            "Processed 360/500 sentences, 4944 pairs\n",
            "Processed 370/500 sentences, 5095 pairs\n",
            "Processed 380/500 sentences, 5242 pairs\n",
            "Processed 390/500 sentences, 5422 pairs\n",
            "Processed 400/500 sentences, 5582 pairs\n",
            "Processed 410/500 sentences, 5763 pairs\n",
            "Processed 430/500 sentences, 6058 pairs\n",
            "Processed 440/500 sentences, 6204 pairs\n",
            "Processed 460/500 sentences, 6490 pairs\n",
            "Processed 470/500 sentences, 6652 pairs\n",
            "Processed 480/500 sentences, 6769 pairs\n",
            "Processed 490/500 sentences, 6895 pairs\n",
            "Processed 500/500 sentences, 6988 pairs\n",
            "\n",
            "Total corpus-variant pairs: 6988\n"
          ]
        }
      ],
      "source": [
        "# STEP 12: Generate variants and extract features\n",
        "\n",
        "print('Generating variants and extracting features...')\n",
        "\n",
        "corpus_features = []\n",
        "variant_features = []\n",
        "pair_count = 0\n",
        "\n",
        "for i, sentence in enumerate(test_sentences):\n",
        "    variants = generate_variants(sentence, MAX_VARIANTS)\n",
        "    if not variants:\n",
        "        continue\n",
        "\n",
        "    # Compute once per original sentence; reuse for all its variants.\n",
        "    corp_feat = extract_features(sentence)\n",
        "\n",
        "    for variant in variants:\n",
        "        var_feat = extract_features(variant)\n",
        "        corpus_features.append(corp_feat)\n",
        "        variant_features.append(var_feat)\n",
        "        pair_count += 1\n",
        "\n",
        "    if (i + 1) % 10 == 0:\n",
        "        print(f'Processed {i+1}/{len(test_sentences)} sentences, {pair_count} pairs')\n",
        "\n",
        "print()\n",
        "print(f'Total corpus-variant pairs: {pair_count}')\n"
      ],
      "id": "E5HBv_I3EYtk"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "rbMvZZZYEYtk"
      },
      "outputs": [],
      "source": [
        "# STEP 13: Classification functions\n",
        "\n",
        "def make_diff(feat1, feat2):\n",
        "    return [feat2[k] - feat1[k] for k in sorted(feat1.keys())]\n",
        "\n",
        "def create_pairs(corpus_feats, variant_feats):\n",
        "    X = []\n",
        "    y = []\n",
        "\n",
        "    for c, v in zip(corpus_feats, variant_feats):\n",
        "        X.append(make_diff(c, v))\n",
        "        y.append(0)\n",
        "\n",
        "    for c, v in zip(corpus_feats, variant_feats):\n",
        "        X.append(make_diff(v, c))\n",
        "        y.append(1)\n",
        "\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "def cross_validate(X, y, n_folds=5):\n",
        "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
        "    accuracies = []\n",
        "    weights = []\n",
        "\n",
        "    for train_idx, test_idx in kf.split(X):\n",
        "        clf = LogisticRegression(solver='lbfgs', max_iter=100)\n",
        "        clf.fit(X[train_idx], y[train_idx])\n",
        "        accuracies.append(clf.score(X[test_idx], y[test_idx]))\n",
        "        weights.append(clf.coef_[0])\n",
        "\n",
        "    return np.mean(accuracies), np.mean(weights, axis=0)"
      ],
      "id": "rbMvZZZYEYtk"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "-ok98F3fEYtk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdaf20b3-ec8d-47ee-ebb0-99e4bed3a6fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "CLASSIFICATION RESULTS (Table 1 Replication)\n",
            "============================================================\n",
            "Lexical Surprisal             : 95.94%  weights: [np.float64(-0.28)]\n",
            "UIDglob (lex)                 : 78.18%  weights: [np.float64(-0.29)]\n",
            "UIDloc (lex)                  : 86.26%  weights: [np.float64(-0.15)]\n",
            "UIDglobNorm (lex)             : 94.75%  weights: [np.float64(-26.1)]\n",
            "UIDlocNorm (lex)              : 95.01%  weights: [np.float64(-14.26)]\n",
            "UIDlocPrevNorm (lex)          : 86.34%  weights: [np.float64(-0.03)]\n",
            "Lex+UIDglob                   : 95.89%  weights: [np.float64(-0.03), np.float64(-0.27)]\n",
            "Lex+UIDloc                    : 96.79%  weights: [np.float64(-0.05), np.float64(-0.24)]\n",
            "Lex+UIDglobNorm               : 95.94%  weights: [np.float64(0.0), np.float64(-0.28)]\n",
            "Lex+UIDlocNorm                : 96.58%  weights: [np.float64(-3.57), np.float64(-0.23)]\n",
            "Syntactic Surprisal           : 90.98%  weights: [np.float64(-0.35)]\n",
            "UIDglob (syn)                 : 91.20%  weights: [np.float64(1.05)]\n",
            "Syn+UIDglob                   : 91.86%  weights: [np.float64(0.36), np.float64(-0.24)]\n"
          ]
        }
      ],
      "source": [
        "# STEP 14: Run classification experiments (Table 1)\n",
        "\n",
        "experiments = [\n",
        "    ('Lexical Surprisal', ['lex_sum']),\n",
        "    ('UIDglob (lex)', ['lex_UIDglob']),\n",
        "    ('UIDloc (lex)', ['lex_UIDloc']),\n",
        "    ('UIDglobNorm (lex)', ['lex_UIDglobNorm']),\n",
        "    ('UIDlocNorm (lex)', ['lex_UIDlocNorm']),\n",
        "    ('UIDlocPrevNorm (lex)', ['lex_UIDlocPrevNorm']),\n",
        "    ('Lex+UIDglob', ['lex_sum', 'lex_UIDglob']),\n",
        "    ('Lex+UIDloc', ['lex_sum', 'lex_UIDloc']),\n",
        "    ('Lex+UIDglobNorm', ['lex_sum', 'lex_UIDglobNorm']),\n",
        "    ('Lex+UIDlocNorm', ['lex_sum', 'lex_UIDlocNorm']),\n",
        "    ('Syntactic Surprisal', ['syn_sum']),\n",
        "    ('UIDglob (syn)', ['syn_UIDglob']),\n",
        "    ('Syn+UIDglob', ['syn_sum', 'syn_UIDglob']),\n",
        "]\n",
        "\n",
        "print('='*60)\n",
        "print('CLASSIFICATION RESULTS (Table 1 Replication)')\n",
        "print('='*60)\n",
        "\n",
        "for name, feat_names in experiments:\n",
        "    corp = [{k: f[k] for k in feat_names} for f in corpus_features]\n",
        "    var = [{k: f[k] for k in feat_names} for f in variant_features]\n",
        "\n",
        "    X, y = create_pairs(corp, var)\n",
        "    acc, w = cross_validate(X, y, n_folds=5)\n",
        "\n",
        "    print(f\"{name:30s}: {acc*100:5.2f}%  weights: {[round(x, 2) for x in w]}\")"
      ],
      "id": "-ok98F3fEYtk"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "DgBQmvDlEYtl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ba61484-8b64-43d8-e725-ecf64ae7cd0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "CORRELATION ANALYSIS (Table 2 Replication)\n",
            "============================================================\n",
            "\n",
            "Lexical Surprisal vs UID Measures:\n",
            "----------------------------------------\n",
            "  UIDglob             : -0.0218\n",
            "  UIDloc              : -0.1868\n",
            "  UIDglobNorm         :  0.3979\n",
            "  UIDlocNorm          :  0.1942\n",
            "  UIDlocPrevNorm      : -0.0261\n",
            "\n",
            "Syntactic Surprisal vs UID Measures:\n",
            "----------------------------------------\n",
            "  UIDglob             : -0.3970\n",
            "  UIDloc              : -0.3886\n",
            "  UIDglobNorm         :  0.1351\n",
            "  UIDlocNorm          :  0.0504\n",
            "  UIDlocPrevNorm      : -0.2026\n"
          ]
        }
      ],
      "source": [
        "# STEP 15: Correlation analysis (Table 2)\n",
        "\n",
        "def pearson_corr(x, y):\n",
        "    cx = [xi for xi, yi in zip(x, y) if not (np.isnan(xi) or np.isnan(yi))]\n",
        "    cy = [yi for xi, yi in zip(x, y) if not (np.isnan(xi) or np.isnan(yi))]\n",
        "    if len(cx) < 2:\n",
        "        return 0\n",
        "    return stats.pearsonr(cx, cy)[0]\n",
        "\n",
        "print('\\n' + '='*60)\n",
        "print('CORRELATION ANALYSIS (Table 2 Replication)')\n",
        "print('='*60)\n",
        "\n",
        "lex_surp = [f['lex_sum'] for f in corpus_features]\n",
        "syn_surp = [f['syn_sum'] for f in corpus_features]\n",
        "\n",
        "print('\\nLexical Surprisal vs UID Measures:')\n",
        "print('-'*40)\n",
        "for name in ['UIDglob', 'UIDloc', 'UIDglobNorm', 'UIDlocNorm', 'UIDlocPrevNorm']:\n",
        "    vals = [f[f'lex_{name}'] for f in corpus_features]\n",
        "    corr = pearson_corr(lex_surp, vals)\n",
        "    print(f\"  {name:20s}: {corr:7.4f}\")\n",
        "\n",
        "print('\\nSyntactic Surprisal vs UID Measures:')\n",
        "print('-'*40)\n",
        "for name in ['UIDglob', 'UIDloc', 'UIDglobNorm', 'UIDlocNorm', 'UIDlocPrevNorm']:\n",
        "    vals = [f[f'syn_{name}'] for f in corpus_features]\n",
        "    corr = pearson_corr(syn_surp, vals)\n",
        "    print(f\"  {name:20s}: {corr:7.4f}\")"
      ],
      "id": "DgBQmvDlEYtl"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wzp55t6EYtl"
      },
      "source": [
        "## Summary\n",
        "\n",
        "This notebook replicates the UID paper:\n",
        "\n",
        "1. Loaded actual Hindi treebank data from your Google Drive\n",
        "2. Trained trigram model for lexical surprisal\n",
        "3. Trained syntactic surprisal model\n",
        "4. Generated variants by permuting preverbal constituents\n",
        "5. Computed all 5 UID measures\n",
        "6. Ran classification experiments\n",
        "7. Computed correlations\n",
        "\n",
        "**Expected findings (from paper):**\n",
        "- Lexical surprisal: ~90% accuracy\n",
        "- UID alone: 50-73% accuracy\n",
        "- Lexical + UID: No significant improvement\n",
        "- UIDglob negatively correlated with surprisal\n",
        "- UIDglobNorm positively correlated\n",
        "\n",
        "**Conclusion:** UID measures don't add value beyond surprisal."
      ],
      "id": "2wzp55t6EYtl"
    }
  ]
}